{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News Category Classification",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7tgJkpoS+NomfzNBnlUze",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vish4github/Projects/blob/master/News%20Category%20Classification%20using%20Transfer%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ysgsRb92F5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install fastai==0.7.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yGfZeiv_pQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install torchvision\n",
        "!pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g41oGTM_p8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.imports import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91RyXf3P_p_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import * \n",
        "!pip install Pillow==4.1.1\n",
        "!pip install image\n",
        "%matplotlib inline\n",
        "from fastai.imports import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmm4A_ZB_qE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install \"torchtext==0.2.3\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddqdVzCt_qH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install spacy \n",
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPS1-KP5ICRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-f_MEa7JALt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n",
        "#torch.backends.cudnn.enabled\n",
        "#!nvidia-smi\n",
        "#!python --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-M8UTiiX7aJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install fastai\n",
        "!pip install fastai --upgrade\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEnMArz1bt5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from fastai.text import Tokenizer, Vocab\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "405IgRcpYPZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/My Drive/Data/News_Category_Dataset_v2.json\"\n",
        "df =  pd.read_json(path,lines=True)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwCva_bMWg1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cates = df.groupby('category')\n",
        "#print(\"total categories:\", cates.ngroups)\n",
        "#print(cates.size())\n",
        "df.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xxUxLi8W5X5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using headlines and short_description as input X\n",
        "\n",
        "df['text'] = df.headline + \" \" + df.short_description\n",
        "\n",
        "# tokenizing\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "#tokenizer.process_text(df.text)\n",
        "tokenizer.fit_on_texts(df.text)\n",
        "X = tokenizer.texts_to_sequences(df.text)\n",
        "df['words'] = X\n",
        "\n",
        "# delete some empty and short data\n",
        "\n",
        "df['word_length'] = df.words.apply(lambda i: len(i))\n",
        "df = df[df.word_length >= 5]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H8GL9h_I8Xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()\n",
        "df = df.drop(df.columns[[1,2,3,4,5,7,8]], axis=1)\n",
        "df.shape\n",
        "df.to_csv('/content/drive/My Drive/Data/cleaned.csv',encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQkC11hLW6vV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = pd.read_csv('/content/drive/My Drive/Data/cleaned.csv')\n",
        "df3 = df2\n",
        "df3.head()\n",
        "\n",
        "X=df3.sample(frac=0.85,random_state=200)\n",
        "test=df3.drop(X.index)\n",
        "train=X.sample(frac=0.85,random_state=200)\n",
        "val=X.drop(train.index)\n",
        "\n",
        "print(train.shape)\n",
        "print(val.shape)\n",
        "print(test.shape)\n",
        "train.to_csv('/content/drive/My Drive/Data/train.csv',encoding='utf-8',index=False)\n",
        "val.to_csv('/content/drive/My Drive/Data/val.csv',encoding='utf-8',index=False)\n",
        "test.to_csv('/content/drive/My Drive/Data/test.csv',encoding='utf-8',index=False)\n",
        "\n",
        "df3.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEMxhQfZLdxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df=pd.read_csv('/content/drive/My Drive/Data/train.csv')\n",
        "train_df=train_df[['category','text']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfibI96dPlZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_df=pd.read_csv('/content/drive/My Drive/Data/val.csv')\n",
        "valid_df=valid_df[['category','text']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgOBgGfRZJkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "#train_df,valid_df=train_test_split(df_text, shuffle=True, test_size=0.25)\n",
        "len(train_df),len(valid_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76fKR-2icNs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head(),valid_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ6TwpNgQbrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs,bptt=64,60"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "989QfD3fRkBn",
        "colab_type": "text"
      },
      "source": [
        "A text is,unlike images which can be fed directly into a model, is composed of words, and we can't apply mathematical functions to them directly. We first have to convert them to numbers. This is done in two differents steps: tokenization and numericalization. A TextDataBunch does all of that behind the scenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfuXglkceKxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Language model data\n",
        "data_lm = TextLMDataBunch.from_df(path,train_df=train_df,valid_df=valid_df,text_cols='text', label_cols='category',bs=bs)\n",
        "\n",
        "# Classification model data\n",
        "data_clas = TextClasDataBunch.from_df(path, train_df=train_df,valid_df=valid_df, vocab=data_lm.train_ds.vocab, text_cols = 'text', label_cols = 'category',bs=bs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daajkKujTWkr",
        "colab_type": "text"
      },
      "source": [
        "Once we have extracted tokens from our texts, we convert to integers by creating a list of all the words used ( Vocab or dictionary). We only keep the ones that appear atleast twice with a maximum vocabulary size of 60,000 (by default) and the ones that don't make the cut are tagged as UNK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyS9EwFLTWvP",
        "colab_type": "text"
      },
      "source": [
        "Once we have extracted tokens from our texts, we convert to integers by creating a list of all the words used ( Vocab or dictionary). We only keep the ones that appear atleast twice with a maximum vocabulary size of 60,000 (by default) and the ones that don't make the cut are tagged as UNK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXZG77aGTWzn",
        "colab_type": "text"
      },
      "source": [
        ".itos and .stoi will give us information on how our tokens and numericalized and stored "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByMN7oo7Tz87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lm.vocab.itos[:10]  # first 10 tokens in our vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhjzrUh1U1IJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lm.vocab.itos[448]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "astpdbGKUBPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lm.train_ds[0][0].data[:10]  # first sentence in our training dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMIBF001U4xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(data_lm.vocab.itos),len(data_lm.train_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4vA3qKHmEpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clas.show_batch()  # shows a portion of the data loaded for the huffpost language model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L4nZabzU7gq",
        "colab_type": "text"
      },
      "source": [
        "The intention is not to train a model that classifies the news categories from scratch. Like in computer vision, we'll use a model pretrained on a bigger dataset (a cleaned subset of wikipedia called wikitext-103). That model has been trained to guess what the next word, its input being all the previous words. It has a recurrent structure and a hidden state that is updated each time it sees a new word. This hidden state thus contains information about the sentence up to that point.\n",
        "\n",
        "We are going to use that 'knowledge' of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because the English of the news headlines and descriptions on huffpost isn't the same as the English of wikipedia, we'll need to adjust a little bit the parameters of our model.Also there might be some words extremely common in that dataset that were barely present in wikipedia, and therefore might no be part of the vocabulary the model was trained on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usz_DUPgeypj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the datasets\n",
        "data_lm.save('/content/drive/My Drive/Data/data_lm_export.pkl')\n",
        "data_clas.save('/content/drive/My Drive/Data/data_clas_export.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoJUAuW8jgQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/Data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOKsOdE7io6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lm = load_data(path, 'data_lm_export.pkl',bs=bs,bptt=bptt)   #bptt = back propagation through time\n",
        "data_clas = load_data(path, 'data_clas_export.pkl', bs=bs)\n",
        "data_bwd = load_data(path, 'data_lm_export.pkl', bs=bs,bptt=bptt, backwards=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYgZ_WDLEiof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_bwd.show_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF_45q_Ce-RY",
        "colab_type": "text"
      },
      "source": [
        "Fine tuning the language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BJxmaoffN2f",
        "colab_type": "text"
      },
      "source": [
        "The idea behind the ULMFit paper is to use transfer learning for this classification task. Our language model isn't randomly initialized but with the weights of a model pretrained on a larger corpus, Wikitext 103. The vocabulary of the two datasets are slightly different, so when loading the weights, we take care to put the embedding weights at the right place, and we rando;ly initiliaze the embeddings for words in the huff post vocab that weren't in the wikitext-103 vocabulary of our pretrained model.\n",
        "\n",
        "This is all done by the first line of code that will download the pretrained model at the first use. The second line is to use Mixed Precision Training, which enables us to use a higher batch size by training part of our model in FP16 precision, and also speeds up training by a factor 2 to 3 on modern GPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBnxvGzPjmRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_model = language_model_learner(data_lm, AWD_LSTM,drop_mult=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBWJmZa_pjWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#learn_model = learn_model.to_fp16(clip=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rgUK8ZT2c3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_model.lr_find() # to find appropriate LR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5ViFNJr2hzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_model.recorder.plot(skip_end=15) # to plot the LR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cx5JCkT2k_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-2\n",
        "lr *= bs/48"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOs_rEdzL5dT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "42f0d992-fa05-4660-a99f-9ec59442f80a"
      },
      "source": [
        "learn_model.fit_one_cycle(1, 1e-2, moms=(0.8,0.7), wd=0.1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.887925</td>\n",
              "      <td>3.716582</td>\n",
              "      <td>0.394290</td>\n",
              "      <td>18:22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVGMyX4ctu2y",
        "colab_type": "text"
      },
      "source": [
        "Now we unfreeze the layers, not just one as in the previous step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdD6a5cEtuCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_model.unfreeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCWwSZpZt4IM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_model.fit_one_cycle(6, lr, moms=(0.8,0.7), wd=0.1)  # we can adjust the learning rate and run the model again if required"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hw6kkLhvUrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#learn_model.fit_one_cycle(1, lr*10, moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gmWn5ipuHRd",
        "colab_type": "text"
      },
      "source": [
        "Now we will save just the encoder which will be later used for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmwZks5vuNf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_model.save_encoder('fwd_enc')\n",
        "#learn_lm.save('fit_1')  # saving the fine tuned language model\n",
        "#learn_lm.load('fit_1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVtOiJb0VfZG",
        "colab_type": "text"
      },
      "source": [
        "Backwards model\n",
        "\n",
        "You can't directly train a bidirectional RNN for language modeling, but you can always enseble a forward and backward model. fastai provides a pretrained forward and backawrd model, so we can repeat the previous step to fine-tune the pretrained backward model. The command language_model_learner checks the data object you pass to automatically decide if it should use the pretrained forward or backward model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh4I4pnrVb2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_model = language_model_learner(data_bwd, AWD_LSTM,drop_mult=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHCU8MQUVxd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_model.fit_one_cycle(1, lr, moms=(0.8,0.7), wd=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtF9C1f_V1aR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_model.unfreeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaDRgDvSV1eP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_model.fit_one_cycle(6, lr, moms=(0.8,0.7),wd=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SC3HXVAWGRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_model.save_encoder('bwd_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VIHtnLAgvGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = data_lm.vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYDfngJggvKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.stoi[\"stingray\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d19LleyIW1ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "awd = learn_model.model[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H57cRO-5hK7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc = learn_model.model[0].encoder\n",
        "enc.weight.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCMz5ul4oRwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.stoi[\"trump\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6tFemWPohd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = \"Trump is\"\n",
        "N_WORDS = 30\n",
        "N_SENTENCES = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtXlHL2Eohon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"\\n\".join(learn_model.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))  #wont make sense since this is the output from the backward model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MmqBGCPtMon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_model.predict(\"This is a review about\", n_words=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gl2HVcWhslm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e46273t_ugpY",
        "colab_type": "text"
      },
      "source": [
        "Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVG2qrWRqVat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs=32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4kLALivp_SV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clas.show_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyPgXOaRqS5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clas_bwd = load_data(path, 'data_clas_export.pkl', bs=bs, backwards=True)   #loading data for the backward classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo4NAKzpqguj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clas_bwd.show_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEB-z32cocwx",
        "colab_type": "text"
      },
      "source": [
        "Fine tuning the forward classifier\n",
        "\n",
        "The classifier needs a little less dropout, so we pass drop_mult=0.5 to multiply all the dropouts by this amount (it's easier than adjusting all the five different values manually). We don't load the pretrained model, but instead our fine-tuned encoder from the previous section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD2aciTwubtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, pretrained=False)\n",
        "learn.load_encoder('fwd_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO_syx50uiB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clas.show_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3nrwvKYulmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-2\n",
        "learn.fit_one_cycle(1, lr, moms=(0.8,0.7), wd=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nF2_UvsutPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.freeze_to(-2)\n",
        "lr /= 2\n",
        "learn.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiIkMiRaEnCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('last2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqbGXHrPEsVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('last2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDMJNoZJuz_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.freeze_to(-3)\n",
        "lr /= 2\n",
        "learn.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H4Wt-qvFD8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('last3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7HLw_LHFGuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('last3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m260FSSGu04B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.unfreeze()\n",
        "lr /= 5\n",
        "learn.fit_one_cycle(10, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVUAvlg62GHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('fwd_clas')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu-eet51O2KT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7dad111a-61bb-4367-8cfb-c5cb05474fe3"
      },
      "source": [
        "preds,targs = learn.get_preds(ordered=True)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgpgo8SjO98q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe47ddcd-dd1f-4e0e-e0c2-5441f1643308"
      },
      "source": [
        "accuracy(preds,targs)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6967)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNnSiJ5KF--Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('fwd_clas')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5cOAgFio1Z9",
        "colab_type": "text"
      },
      "source": [
        "Same thing backwards\n",
        "\n",
        "Then we do the same thing for the backward model, the only thigns to adjust are the names of the data object and the fine-tuned encoder we load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RafFwojiFbmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_bwd = text_classifier_learner(data_clas_bwd, AWD_LSTM, drop_mult=0.5, pretrained=False)\n",
        "learn_bwd.load_encoder('bwd_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAloyLuApAOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-2\n",
        "learn_bwd.fit_one_cycle(1, lr, moms=(0.8,0.7), wd=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VN8SMZwpG47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_bwd.freeze_to(-2)\n",
        "lr /= 2\n",
        "learn_bwd.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBbEj8KAPN5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_bwd.save('twolayers_bwd')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVdO69HSPUVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_bwd.load('twolayers_bwd')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BArgd8gpQXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_bwd.freeze_to(-3)\n",
        "lr /= 2\n",
        "learn_bwd.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQSeFo0FdNfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_bwd.save('threelayer_bwd')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwgZylMSdSmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_bwd.load('threelayer_bwd')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd2wB_FNpSK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_bwd.unfreeze()\n",
        "lr /= 5\n",
        "learn_bwd.fit_one_cycle(10, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqrd7oByeJPd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9b47f82-919f-4b86-ba8c-fee177e4f20c"
      },
      "source": [
        "preds_bwd,targs_bwd = learn_bwd.get_preds(ordered=True)\n",
        "accuracy(preds_bwd,targs_bwd)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6994)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mij4E2fSpWL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_bwd.save('bwd_clas')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMS06JdSpafP",
        "colab_type": "text"
      },
      "source": [
        "Ensembling the two models\n",
        "\n",
        "\n",
        "For our final results, we'll take the average of the predictions of the forward and the backward models. SInce the samples are sorted by text lengths for batching, we pass the argument ordered=True to get the predictions in the order of the texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kOGkdUipfU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_fwd,lbl_fwd = learn.get_preds(ordered=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTWtJmXVph0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_bwd,lbl_bwd = learn_bwd.get_preds(ordered=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcUO-_F4pkMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_pred = (pred_fwd+pred_bwd)/2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kESFytJpq7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy(final_pred, lbl_fwd)\n",
        "#tensor(0.7141)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P192whqcVp8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}